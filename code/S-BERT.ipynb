{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Task 2. Sentence Embedding with Sentence BERT (3 Points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sentence-BERT](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "\n",
    "[Reference Code](https://www.pinecone.io/learn/series/nlp/train-sentence-transformers-softmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "import pickle\n",
    "\n",
    "# Set GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import custom modules and classes\n",
    "from myutils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset use MNLI only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNLI dataset\n",
    "mnli = datasets.load_dataset('glue', 'mnli')\n",
    "mnli['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(mnli['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset dictionary with subset of data from MNLI\n",
    "from datasets import DatasetDict\n",
    "\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': mnli['train'].shuffle(seed=55).select(list(range(3000))),\n",
    "    'test': mnli['test_mismatched'].shuffle(seed=55).select(list(range(500))),\n",
    "    'validation': mnli['validation_mismatched'].shuffle(seed=55).select(list(range(1000)))\n",
    "})\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the model weights saved from the pretrained model in Bert-update.ipynb\n",
    "\n",
    "data     = pickle.load(open('../models/bert_model_data.pkl', 'rb'))\n",
    "\n",
    "vocab_size          = data['vocab_size']\n",
    "word2id             = data['word2id']\n",
    "batch_size          = data['batch_size']\n",
    "max_mask            = data['max_mask']\n",
    "max_len             = data['max_len']\n",
    "n_layers            = data['n_layers']\n",
    "n_heads             = data['n_heads']\n",
    "d_model             = data['d_model']\n",
    "d_ff                = data['d_ff']\n",
    "d_k                 = data['d_k']\n",
    "d_v                 = data['d_v']\n",
    "n_segments          = data['n_segments']\n",
    "word_list           = data['word_list']\n",
    "id2word             = data['id2word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocessing function for tokenizing and preparing input data for my model\n",
    "def preprocess_function(examples):\n",
    "    lst_input_ids_premise = []\n",
    "    lst_input_ids_hypothesis = []\n",
    "    lst_masked_tokens_premise = []\n",
    "    lst_masked_pos_premise = []\n",
    "    lst_masked_tokens_hypothesis = []\n",
    "    lst_masked_pos_hypothesis = []\n",
    "    lst_segment_ids = []\n",
    "    lst_attention_premise=[]\n",
    "    lst_attention_hypothesis=[]\n",
    "    labels = examples['label']\n",
    "    max_seq_length = 200\n",
    "    seed(55) \n",
    "    for i in range(len(examples['premise'])):\n",
    "\n",
    "        # convert the word to numeric\n",
    "        tokens_premise, tokens_hypothesis            = [word2id[word] if word in word_list else len(word_list) for word in examples['premise'][i].split()], \\\n",
    "                                                    [word2id[word] if word in word_list else len(word_list) for word in examples['hypothesis'][i].split()]\n",
    "        \n",
    "        #1. token embedding - add CLS and SEP on beginning and ending of premise and hypothesis\n",
    "        input_ids_premise = [word2id['[CLS]']] + tokens_premise + [word2id['[SEP]']]\n",
    "        input_ids_hypothesis = [word2id['[CLS]']] + tokens_hypothesis + [word2id['[SEP]']]\n",
    "      \n",
    "        #2. segment embedding - there one sentence so I decide to segment it as all 0\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        #3 masking\n",
    "        n_pred_premise = min(max_mask, max(1, int(round(len(input_ids_premise) * 0.15))))\n",
    "\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos_premise = [i for i, token in enumerate(input_ids_premise) if token != word2id['[CLS]'] \n",
    "                                 and token != word2id['[SEP]']]\n",
    "        shuffle(candidates_masked_pos_premise)\n",
    "        masked_tokens_premise, masked_pos_premise = [], [] #compare the output with masked_tokens\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos_premise[:n_pred_premise]:\n",
    "            masked_pos_premise.append(pos)\n",
    "            masked_tokens_premise.append(input_ids_premise[pos])\n",
    "           \n",
    "            if random() < 0.1:  #10% replace with random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids_premise[pos] = word2id[id2word[index]]\n",
    "            elif random() < 0.8:  #80 replace with [MASK]\n",
    "                input_ids_premise[pos] = word2id['[MASK]']\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "        n_pred_hypothesis = min(max_mask, max(1, int(round(len(input_ids_hypothesis) * 0.15))))\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos_hypothesis = [i for i, token in enumerate(input_ids_hypothesis) if token != word2id['[CLS]'] \n",
    "                                 and token != word2id['[SEP]']]\n",
    "        shuffle(candidates_masked_pos_hypothesis)\n",
    "        masked_tokens_hypothesis, masked_pos_hypothesis = [], [] #compare the output with masked_tokens\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos_hypothesis[:n_pred_hypothesis]:\n",
    "            masked_pos_hypothesis.append(pos)\n",
    "            masked_tokens_hypothesis.append(input_ids_hypothesis[pos])\n",
    "            if random() < 0.1:  #10% replace with random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids_hypothesis[pos] = word2id[id2word[index]]\n",
    "            elif random() < 0.8:  #80 replace with [MASK]\n",
    "                input_ids_hypothesis[pos] = word2id['[MASK]']\n",
    "            else: \n",
    "                pass\n",
    "        \n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad_premise = max_seq_length - len(input_ids_premise)\n",
    "        input_ids_premise.extend([0] * n_pad_premise)\n",
    "        \n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred_premise:\n",
    "            n_pad_premise = max_mask - n_pred_premise\n",
    "            masked_tokens_premise.extend([0] * n_pad_premise)\n",
    "            masked_pos_premise.extend([0] * n_pad_premise)\n",
    "            attention_premise = [1]*n_pred_premise+[0]*(n_pad_premise)\n",
    "            \n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad_hypothesis = max_seq_length - len(input_ids_hypothesis)\n",
    "        input_ids_hypothesis.extend([0] * n_pad_hypothesis)\n",
    "        \n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred_hypothesis:\n",
    "            n_pad_hypothesis = max_mask - n_pred_hypothesis\n",
    "            masked_tokens_hypothesis.extend([0] * n_pad_hypothesis)\n",
    "            masked_pos_hypothesis.extend([0] * n_pad_hypothesis)\n",
    "            attention_hypothesis = [1]*n_pred_hypothesis+[0]*(n_pad_hypothesis)\n",
    "        \n",
    "        # add the value to own list\n",
    "        lst_input_ids_premise.append(input_ids_premise)\n",
    "        lst_input_ids_hypothesis.append(input_ids_hypothesis)\n",
    "        lst_segment_ids.append(segment_ids)\n",
    "        lst_masked_tokens_premise.append(masked_tokens_premise)\n",
    "        lst_masked_pos_premise.append(masked_pos_premise)\n",
    "        lst_masked_tokens_hypothesis.append(masked_tokens_hypothesis)\n",
    "        lst_masked_pos_hypothesis.append(masked_pos_hypothesis)\n",
    "        lst_attention_premise.append(attention_premise)\n",
    "        lst_attention_hypothesis.append(attention_hypothesis)\n",
    "\n",
    "    # return as a dictionary\n",
    "    return {\n",
    "        \"premise_input_ids\": lst_input_ids_premise,\n",
    "        \"premise_pos_mask\":lst_masked_pos_premise,\n",
    "        \"hypothesis_input_ids\": lst_input_ids_hypothesis,\n",
    "        \"hypothesis_pos_mask\": lst_masked_pos_hypothesis,\n",
    "        \"segment_ids\": lst_segment_ids,\n",
    "        \"attention_premise\": lst_attention_premise,\n",
    "        \"attention_hypothesis\": lst_attention_hypothesis,\n",
    "        \"labels\" : labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bab8f82308347ef960ec26e3f08befd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e95c79591bb43f5855d4c1e10e34720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map raw dataset with preprocess_function to create new data dict\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['premise','hypothesis','label'])\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_pos_mask', 'hypothesis_input_ids', 'hypothesis_pos_mask', 'segment_ids', 'attention_premise', 'attention_hypothesis', 'labels'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_pos_mask', 'hypothesis_input_ids', 'hypothesis_pos_mask', 'segment_ids', 'attention_premise', 'attention_hypothesis', 'labels'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_pos_mask', 'hypothesis_input_ids', 'hypothesis_pos_mask', 'segment_ids', 'attention_premise', 'attention_hypothesis', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 200])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 200])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 200])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# print the shape of each key \n",
    "for batch in train_dataloader:\n",
    "    print(batch['premise_input_ids'].shape)\n",
    "    print(batch['premise_pos_mask'].shape)\n",
    "    print(batch['hypothesis_input_ids'].shape)\n",
    "    print(batch['hypothesis_pos_mask'].shape)\n",
    "    print(batch['segment_ids'].shape)\n",
    "    print(batch['attention_premise'].shape)\n",
    "    print(batch['attention_hypothesis'].shape)\n",
    "    print(batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(60305, 768)\n",
       "    (pos_embed): Embedding(1000, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=60305, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start from the pretrained model in Bert-update.ipynb\n",
    "model = BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ")\n",
    "model.load_state_dict(torch.load('../models/bert_model.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "## Classification Objective Function \n",
    "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t âˆˆ  \\mathbb{R}^{3n \\times k}  $:\n",
    "\n",
    "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
    "\n",
    "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
    "\n",
    "## Regression Objective Function. \n",
    "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
    "\n",
    "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u,v):\n",
    "    # build the |u-v| tensor\n",
    "    uv = torch.sub(u, v)   # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "    \n",
    "    # concatenate u, v, |u-v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(vocab_size*3, 3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIT (DSAI)\\Spring 2025\\NLP\\nlp-env\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(raw_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "\t\toptimizer_classifier, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37752330d13441ab17fa26d84fd3819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 11.070644\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7927a1b75d2241bc96d1b2754dc77d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 6.145179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ee3cd512864bf2ae466f9acb15d2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 12.418708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bf35e622d04728ac8b4f83c79bf353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = 8.316485\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9115becfa64502a0095ed87990352b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | loss = 4.723993\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epoch = 5\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()  \n",
    "    classifier_head.train()\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "        \n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        pos_mask_a = batch['premise_pos_mask'].to(device)\n",
    "        pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "        attention_a = batch['attention_premise'].to(device)\n",
    "        attention_b = batch['attention_hypothesis'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "        \n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u, _ = model(inputs_ids_a, segment_ids, pos_mask_a)  \n",
    "        v, _ = model(inputs_ids_b, segment_ids, pos_mask_b)  \n",
    "\n",
    "        u_last_hidden_state = u # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v_last_hidden_state = v # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "        # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
    "        \n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "        \n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "        \n",
    "        # process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x) #batch_size, classifer\n",
    "        \n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, label)\n",
    "        \n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        scheduler.step() # update learning rate scheduler\n",
    "        scheduler_classifier.step()\n",
    "        \n",
    "    print(f'Epoch: {epoch + 1} | loss = {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model.eval()\n",
    "classifier_head.eval()\n",
    "total_similarity = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # Prepare batches and move all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        pos_mask_a = batch['premise_pos_mask'].to(device)\n",
    "        pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "        attention_a = batch['attention_premise'].to(device)\n",
    "        attention_b = batch['attention_hypothesis'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "        \n",
    "        # Extract token embeddings from BERT at last_hidden_state\n",
    "        u, _ = model(inputs_ids_a, segment_ids, pos_mask_a)  \n",
    "        v, _ = model(inputs_ids_b, segment_ids, pos_mask_b)  \n",
    "        \n",
    "        # Get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u, attention_a)  # Shape: [batch_size, hidden_dim]\n",
    "        v_mean_pool = mean_pool(v, attention_b)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Move tensors to CPU and convert to NumPy for cosine similarity calculation\n",
    "        u_mean_pool = u_mean_pool.cpu().numpy()  # Convert to NumPy array\n",
    "        v_mean_pool = v_mean_pool.cpu().numpy()  # Convert to NumPy array\n",
    "        \n",
    "        # Calculate cosine similarity for each pair in the batch\n",
    "        similarity_scores = cosine_similarity(u_mean_pool, v_mean_pool)  # Shape: [batch_size, batch_size]\n",
    "        \n",
    "        # Extract the diagonal (similarity between corresponding pairs)\n",
    "        batch_similarity = np.diag(similarity_scores).mean()  # Average similarity for the batch\n",
    "        total_similarity += batch_similarity\n",
    "    \n",
    "# Calculate the average similarity across all batches\n",
    "average_similarity = total_similarity / len(eval_dataloader)\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentence of model\n",
    "def tokenize_sentence_model(sentence_a, sentence_b):\n",
    "    lst_input_ids_premise = []\n",
    "    lst_input_ids_hypothesis = []\n",
    "    lst_masked_tokens_premise = []\n",
    "    lst_masked_pos_premise = []\n",
    "    lst_masked_tokens_hypothesis = []\n",
    "    lst_masked_pos_hypothesis = []\n",
    "    lst_segment_ids = []\n",
    "    lst_attention_premise=[]\n",
    "    lst_attention_hypothesis=[]\n",
    "    max_seq_length = 200\n",
    "    seed(55) \n",
    "\n",
    "    tokens_premise, tokens_hypothesis            = [word2id[word] if word in word_list else len(word_list) for word in sentence_a.split()], \\\n",
    "                                                    [word2id[word] if word in word_list else len(word_list) for word in sentence_b.split()]\n",
    "    \n",
    "    input_ids_premise = [word2id['[CLS]']] + tokens_premise + [word2id['[SEP]']]\n",
    "    input_ids_hypothesis = [word2id['[CLS]']] + tokens_hypothesis + [word2id['[SEP]']]\n",
    "    \n",
    "    #2. segment embedding \n",
    "    segment_ids = [0] * max_seq_length\n",
    "     #3 masking\n",
    "    n_pred_premise = min(max_mask, max(1, int(round(len(input_ids_premise) * 0.15))))\n",
    "\n",
    "    #get all the pos excluding CLS and SEP\n",
    "    candidates_masked_pos_premise = [i for i, token in enumerate(input_ids_premise) if token != word2id['[CLS]'] \n",
    "                                 and token != word2id['[SEP]']]\n",
    "    shuffle(candidates_masked_pos_premise)\n",
    "    masked_tokens_premise, masked_pos_premise = [], [] #compare the output with masked_tokens\n",
    "    #simply loop and mask accordingly\n",
    "    for pos in candidates_masked_pos_premise[:n_pred_premise]:\n",
    "        masked_pos_premise.append(pos)\n",
    "        masked_tokens_premise.append(input_ids_premise[pos])\n",
    "           \n",
    "        if random() < 0.1:  #10% replace with random token\n",
    "            index = randint(0, vocab_size - 1)\n",
    "            input_ids_premise[pos] = word2id[id2word[index]]\n",
    "        elif random() < 0.8:  #80 replace with [MASK]\n",
    "            input_ids_premise[pos] = word2id['[MASK]']\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    n_pred_hypothesis = min(max_mask, max(1, int(round(len(input_ids_hypothesis) * 0.15))))\n",
    "    #get all the pos excluding CLS and SEP\n",
    "    candidates_masked_pos_hypothesis = [i for i, token in enumerate(input_ids_hypothesis) if token != word2id['[CLS]'] \n",
    "                                 and token != word2id['[SEP]']]\n",
    "    shuffle(candidates_masked_pos_hypothesis)\n",
    "    masked_tokens_hypothesis, masked_pos_hypothesis = [], [] #compare the output with masked_tokens\n",
    "    #simply loop and mask accordingly\n",
    "    for pos in candidates_masked_pos_hypothesis[:n_pred_hypothesis]:\n",
    "        masked_pos_hypothesis.append(pos)\n",
    "        masked_tokens_hypothesis.append(input_ids_hypothesis[pos])\n",
    "        if random() < 0.1:  #10% replace with random token\n",
    "            index = randint(0, vocab_size - 1)\n",
    "            input_ids_hypothesis[pos] = word2id[id2word[index]]\n",
    "        elif random() < 0.8:  #80 replace with [MASK]\n",
    "            input_ids_hypothesis[pos] = word2id['[MASK]']\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    #4. pad the sentence to the max length\n",
    "    n_pad_premise = max_seq_length - len(input_ids_premise)\n",
    "    input_ids_premise.extend([0] * n_pad_premise)\n",
    "        \n",
    "    #5. pad the mask tokens to the max length\n",
    "    if max_mask > n_pred_premise:\n",
    "        n_pad_premise = max_mask - n_pred_premise\n",
    "        masked_tokens_premise.extend([0] * n_pad_premise)\n",
    "        masked_pos_premise.extend([0] * n_pad_premise)\n",
    "        attention_premise = [1]*n_pred_premise+[0]*(n_pad_premise)\n",
    "            \n",
    "    #4. pad the sentence to the max length\n",
    "    n_pad_hypothesis = max_seq_length - len(input_ids_hypothesis)\n",
    "    input_ids_hypothesis.extend([0] * n_pad_hypothesis)\n",
    "        \n",
    "    #5. pad the mask tokens to the max length\n",
    "    if max_mask > n_pred_hypothesis:\n",
    "        n_pad_hypothesis = max_mask - n_pred_hypothesis\n",
    "        masked_tokens_hypothesis.extend([0] * n_pad_hypothesis)\n",
    "        masked_pos_hypothesis.extend([0] * n_pad_hypothesis)\n",
    "        attention_hypothesis = [1]*n_pred_hypothesis+[0]*(n_pad_hypothesis)\n",
    "\n",
    "    lst_input_ids_premise.append(input_ids_premise)\n",
    "    lst_input_ids_hypothesis.append(input_ids_hypothesis)\n",
    "    lst_segment_ids.append(segment_ids)\n",
    "    lst_masked_tokens_premise.append(masked_tokens_premise)\n",
    "    lst_masked_pos_premise.append(masked_pos_premise)\n",
    "    lst_masked_tokens_hypothesis.append(masked_tokens_hypothesis)\n",
    "    lst_masked_pos_hypothesis.append(masked_pos_hypothesis)\n",
    "    lst_attention_premise.append(attention_premise)\n",
    "    lst_attention_hypothesis.append(attention_hypothesis)\n",
    "\n",
    "    return {\n",
    "        \"premise_input_ids\": lst_input_ids_premise,\n",
    "        \"premise_pos_mask\":lst_masked_pos_premise,\n",
    "        \"hypothesis_input_ids\": lst_input_ids_hypothesis,\n",
    "        \"hypothesis_pos_mask\": lst_masked_pos_hypothesis,\n",
    "        \"segment_ids\": lst_segment_ids,\n",
    "        \"attention_premise\": lst_attention_premise,\n",
    "        \"attention_hypothesis\": lst_attention_hypothesis,\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9999\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity_model(model, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs = tokenize_sentence_model(sentence_a, sentence_b)\n",
    "\n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "    inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "    pos_mask_a = batch['premise_pos_mask'].to(device)\n",
    "    pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
    "    segment_ids = batch['segment_ids'].to(device)\n",
    "    attention_a = batch['attention_premise'].to(device)\n",
    "    attention_b = batch['attention_hypothesis'].to(device)\n",
    "    \n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    u,_ = model(inputs_ids_a, segment_ids, pos_mask_a)  \n",
    "    v,_ = model(inputs_ids_b, segment_ids, pos_mask_b) \n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # Move to CPU for NumPy\n",
    "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # Move to CPU for NumPy\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage:\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "similarity = calculate_similarity_model(model, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence_a = 'lemon'\n",
    "sentence_b = 'lime'\n",
    "similarity = calculate_similarity_model(model, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(model.state_dict(), '../models/S_BERT.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Evaluation and Analysis (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to calculate the total parameters in each model\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the evaluation loss for each model\n",
    "def calculate_loss_model(model, classifier, criterion, eval_dataloader):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "\n",
    "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "            pos_mask_a = batch['premise_pos_mask'].to(device)\n",
    "            pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
    "            segment_ids = batch['segment_ids'].to(device)\n",
    "            attention_a = batch['attention_premise'].to(device)\n",
    "            attention_b = batch['attention_hypothesis'].to(device)\n",
    "            label = batch['labels'].to(device)\n",
    "\n",
    "            # extract token embeddings from BERT at last_hidden_state\n",
    "            u, _ = model(inputs_ids_a, segment_ids, pos_mask_a)  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "            v, _ = model(inputs_ids_b, segment_ids, pos_mask_b)  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "            # get the mean pooled vectors\n",
    "            u_mean_pool = mean_pool(u, attention_a) # batch_size, hidden_dim\n",
    "            v_mean_pool = mean_pool(v, attention_b) # batch_size, hidden_dim\n",
    "\n",
    "            # build the |u-v| tensor\n",
    "            uv = torch.sub(u_mean_pool, v_mean_pool)   # batch_size,hidden_dim\n",
    "            uv_abs = torch.abs(uv) # batch_size,hidden_dim\n",
    "            \n",
    "            # concatenate u, v, |u-v|\n",
    "            x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1) # batch_size, 3*hidden_dim\n",
    "            \n",
    "            # process concatenated tensor through classifier_head\n",
    "            x = classifier(x) #batch_size, classifer\n",
    "            \n",
    "            # calculate the 'softmax-loss' between predicted and true label\n",
    "            loss = criterion(x, label)\n",
    "\n",
    "            total_loss += loss\n",
    "    \n",
    "    average_loss = total_loss/len(eval_dataloader)\n",
    "    print(f\"Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to compute the cosine similarity of model\n",
    "def calculate_cosine_sim_model(model, classifier,eval_dataloader):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    total_similarity = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            # prepare batches and more all to the active device\n",
    "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "            pos_mask_a = batch['premise_pos_mask'].to(device)\n",
    "            pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
    "            segment_ids = batch['segment_ids'].to(device)\n",
    "            attention_a = batch['attention_premise'].to(device)\n",
    "            attention_b = batch['attention_hypothesis'].to(device)\n",
    "            label = batch['labels'].to(device)\n",
    "\n",
    "            # extract token embeddings from BERT at last_hidden_state\n",
    "\n",
    "            u, _ = model(inputs_ids_a, segment_ids, pos_mask_a)  \n",
    "            v, _ = model(inputs_ids_b, segment_ids, pos_mask_b) \n",
    "            # get the mean pooled vectors\n",
    "            u_mean_pool = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
    "            v_mean_pool = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1) # batch_size, hidden_dim\n",
    "\n",
    "            similarity_score = cosine_similarity(u_mean_pool.reshape(1, -1), v_mean_pool.reshape(1, -1))[0, 0]\n",
    "            total_similarity += similarity_score\n",
    "        \n",
    "    average_similarity = total_similarity / len(eval_dataloader)\n",
    "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create function for compute consine similarity of unseen 2 sentence \n",
    "def calculate_similarity_model(model, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs = tokenize_sentence_model(sentence_a, sentence_b)\n",
    "    \n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = torch.tensor(inputs['premise_input_ids']).to(device)\n",
    "    pos_mask_a = torch.tensor(inputs['premise_pos_mask']).to(device)\n",
    "    attention_a = torch.tensor(inputs['attention_premise']).to(device)\n",
    "    inputs_ids_b = torch.tensor(inputs['hypothesis_input_ids']).to(device)\n",
    "    pos_mask_b = torch.tensor(inputs['hypothesis_pos_mask']).to(device)\n",
    "    attention_b = torch.tensor(inputs['attention_hypothesis']).to(device)\n",
    "    segment = torch.tensor(inputs['segment_ids']).to(device)\n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    u,_ = model(inputs_ids_a, segment, pos_mask_a)  \n",
    "    v,_ = model(inputs_ids_b, segment, pos_mask_b) \n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  \n",
    "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  \n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
    "\n",
    "    return similarity_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalue model to return accuracy, precision, recall, and F1-score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "\n",
    "def evaluate_nli_model(model, classifier, eval_dataloader, device):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "            inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "            pos_mask_a = batch['premise_pos_mask'].to(device)\n",
    "            pos_mask_b = batch['hypothesis_pos_mask'].to(device)\n",
    "            segment_ids = batch['segment_ids'].to(device)\n",
    "            attention_a = batch['attention_premise'].to(device)\n",
    "            attention_b = batch['attention_hypothesis'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Extract token embeddings\n",
    "            u, _ = model(inputs_ids_a, segment_ids, pos_mask_a)\n",
    "            v, _ = model(inputs_ids_b, segment_ids, pos_mask_b)\n",
    "            \n",
    "            # Mean pooling\n",
    "            u_mean_pool = mean_pool(u, attention_a)\n",
    "            v_mean_pool = mean_pool(v, attention_b)\n",
    "            \n",
    "            # Compute absolute difference\n",
    "            uv_abs = torch.abs(u_mean_pool - v_mean_pool)\n",
    "            \n",
    "            # Concatenate u, v, and |u-v|\n",
    "            x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1)\n",
    "            \n",
    "            # Pass through classifier\n",
    "            logits = classifier(x)\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-score: {f1:.4f}')\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def predict_nli_and_similarity(model, classifier_head, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs = tokenize_sentence_model(sentence_a, sentence_b)\n",
    "    \n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = torch.tensor(inputs['premise_input_ids']).to(device)\n",
    "    pos_mask_a = torch.tensor(inputs['premise_pos_mask']).to(device)\n",
    "    attention_a = torch.tensor(inputs['attention_premise']).to(device)\n",
    "    inputs_ids_b = torch.tensor(inputs['hypothesis_input_ids']).to(device)\n",
    "    pos_mask_b = torch.tensor(inputs['hypothesis_pos_mask']).to(device)\n",
    "    attention_b = torch.tensor(inputs['attention_hypothesis']).to(device)\n",
    "    segment = torch.tensor(inputs['segment_ids']).to(device)\n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    with torch.no_grad():\n",
    "        u, _ = model(inputs_ids_a, segment, pos_mask_a)\n",
    "        v, _ = model(inputs_ids_b, segment, pos_mask_b)\n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a)\n",
    "    v = mean_pool(v, attention_b)\n",
    "\n",
    "    # Convert to numpy for cosine similarity\n",
    "    u_np = u.cpu().numpy().reshape(-1)\n",
    "    v_np = v.cpu().numpy().reshape(-1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u_np.reshape(1, -1), v_np.reshape(1, -1))[0, 0]\n",
    "\n",
    "    # Compute NLI classification\n",
    "    uv_abs = torch.abs(u - v)  # |u - v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1)  # Concatenate for classification\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = classifier_head(x)  # Pass through classification head\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # NLI labels: contradiction (0), neutral (1), entailment (2)\n",
    "    labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "    nli_result = labels[torch.argmax(probabilities).item()]\n",
    "\n",
    "    return similarity_score, nli_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u shape: torch.Size([8, 5, 60305]), v shape: torch.Size([8, 5, 60305])\n"
     ]
    }
   ],
   "source": [
    "# print shapes\n",
    "print(f\"u shape: {u.shape}, v shape: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model in Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(60305, 768)\n",
       "    (pos_embed): Embedding(1000, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=60305, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model_before for Task 1\n",
    "model_before= BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ")\n",
    "model_before.load_state_dict(torch.load('../models/bert_model.pth'))\n",
    "model_before.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "126260371\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "calculate_cosine_sim_model(model_before,classifier_head,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 7.1776\n"
     ]
    }
   ],
   "source": [
    "calculate_loss_model(model_before,classifier_head,criterion,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "sentence_a = 'A man is playing basketball on stage'\n",
    "sentence_b = \"The man is exercising\"\n",
    "similarity = calculate_similarity_model(model_before, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EvaluateModel_after for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): Embedding(\n",
       "    (tok_embed): Embedding(60305, 768)\n",
       "    (pos_embed): Embedding(1000, 768)\n",
       "    (seg_embed): Embedding(2, 768)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x EncoderLayer(\n",
       "      (enc_self_attn): MultiHeadAttention(\n",
       "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (pos_ffn): PoswiseFeedForwardNet(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activ): Tanh()\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (decoder): Linear(in_features=768, out_features=60305, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model_after for Task 2\n",
    "model_after= BERT(\n",
    "    n_layers, \n",
    "    n_heads, \n",
    "    d_model, \n",
    "    d_ff, \n",
    "    d_k, \n",
    "    n_segments, \n",
    "    vocab_size, \n",
    "    max_len, \n",
    "    device\n",
    ")\n",
    "model_after.load_state_dict(torch.load('../models/S_BERT.pt'))\n",
    "model_after.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "calculate_cosine_sim_model(model_after,classifier_head,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 7.1917\n"
     ]
    }
   ],
   "source": [
    "calculate_loss_model(model_after,classifier_head,criterion,eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "sentence_a = 'A man is playing basketball on stage'\n",
    "sentence_b = \"The man is exercising\"\n",
    "similarity = calculate_similarity_model(model_after, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3320\n",
      "Precision: 0.1102\n",
      "Recall: 0.3320\n",
      "F1-score: 0.1655\n",
      "{'accuracy': 0.332, 'precision': 0.110224, 'recall': 0.332, 'f1': 0.1655015015015015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIT (DSAI)\\Spring 2025\\NLP\\nlp-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = evaluate_nli_model(model_after, classifier_head, eval_dataloader, device)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 1.0000\n",
      "NLI Prediction: neutral\n"
     ]
    }
   ],
   "source": [
    "sentence_a = 'A man is playing basketball on stage'\n",
    "sentence_b = \"The man is exercising\"\n",
    "similarity, nli_result = predict_nli_and_similarity(model, classifier_head, sentence_a, sentence_b, device)\n",
    "\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "print(f\"NLI Prediction: {nli_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "| **Model Type**       | **MNLI Performance**                                                                 |\n",
    "|-----------------------|-------------------------------------------------------------------------------------|\n",
    "| **Our Model**         | Accuracy: 0.3320, Precision: 0.1102, Recall: 0.3320, F1-Score: 0.1655              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
